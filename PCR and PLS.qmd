---
title: "PCR and PLS"
author: "Pan Phyu Phyu Hmwe"
format: docx
editor: visual
---

## Principal Component Regression (PCR)

The College dataset provides information about U.S. colleges and universities, including various metrics that describe their characteristics. The data were likely collected from publicly available records and surveys conducted by educational institutions.

We are using Principal Component Regression (PCR) on the College dataset to understand and model the factors influencing the graduation rate (Grad.Rate) of U.S. colleges and universities. By applying PCR, we aim to address potential challenges with high-dimensional data, such as multicollinearity, while reducing the complexity of the model.

```{r}
library(ISLR2)
library(dplyr)
library(tidyr)

#load the dataset
data("College")
college.df <- College
head(college.df)
```

We are converting Private to a numeric variable for regression.

```{r}
# Convert 'Private' to character
college.df$Private <- as.character(college.df$Private)

#Convert 'Private' to a numeric variable
college.df$Private <- ifelse(college.df$Private == "Yes", 1, 0)

head(college.df)

```

Then, we will fit the PCR model using cross-validation using Grad.Rate as a response variable. Scale is set to true to ensure that all predictors are on the same scale.

```{r}
library(pls)
set.seed(1)

# Fit PCR model using cross-validation
pcr_model <- pcr(Grad.Rate ~ ., data = college.df,scale = TRUE, validation = "CV")

# Summary to see cross-validation results
summary(pcr_model)

```

We will now plot the cross validation scores to choose the optimal number of components.

```{r}
# Plot cross-validation MSE 
validationplot(pcr_model, val.type = "MSEP")
```

The cross-validation plot shows a sharp decrease in MSEP from 1 to 3 components, suggesting these capture key patterns. Beyond 3, the MSEP levels off, indicating diminishing returns. From the summary, M = 1 explains 30.61% of predictor variance and 10.58% of Grad.Rate, while M = 7 captures 85.96% of predictor variance and 42.23% of Grad.Rate. Using M = 17 (all components) explains 100% of the predictors' variance but only 46.15% of Grad.Rate. Therefore, 3 to 7 components offer a good balance between simplicity and accuracy.

We will now do the PCR on training data and make a plot to assess its performance.

```{r}
set.seed(1)

# Split the College data into training and testing sets (50% each)
train <- college.df %>%
  sample_frac(0.5)

test <- college.df %>%
  setdiff(train)

```

```{r}
# Fit PCR model on training data with cross-validation
pcr_fit2 <- pcr(Grad.Rate ~ ., data = train, scale = TRUE, validation = "CV")

# Plot the cross-validation MSEP to choose the optimal number of components
validationplot(pcr_fit2, val.type = "MSEP")

```

The optimal number of components, M, for PCR model appears to be 5, as this is where the cross-validation error is minimized. Adding more components beyond this point does not provide a significant reduction in error, suggesting that 5 components will strike a good balance between model complexity and predictive accuracy.

We implement the test MSE using M=5 as below:

```{r}
# Create model matrices for PCR
x_train <- model.matrix(Grad.Rate ~ ., train)[, -1]  # Exclude intercept
x_test <- model.matrix(Grad.Rate ~ ., test)[, -1]    # Exclude intercept

# Extract response variables
y_train <- train %>%
  select(Grad.Rate) %>%
  unlist() %>%
  as.numeric()

y_test <- test %>%
  select(Grad.Rate) %>%
  unlist() %>%
  as.numeric()

# Fit PCR model with the optimal number of components identified from cross-validation (M = 5)

pcr_pred <- predict(pcr_fit2, x_test, ncomp = 5)

mean((pcr_pred - y_test)^2)
```

The test Mean Squared Error (MSE) of 181.3046 indicates the average squared difference between the predicted and actual Grad.Rate values on the test set, reflecting the performance of PCR model.

Since we have tested the performance of the test set, we will do the PCR fitting on a full dataset again using 5 components.

```{r}
# Create model matrix for the full dataset
x_full <- model.matrix(Grad.Rate ~ ., college.df)[, -1]

# Extract the response variable for the full dataset
y_full <- college.df %>%
  select(Grad.Rate) %>%
  unlist() %>%
  as.numeric()

# Fit PCR model on the full dataset using M = 5 components
pcr_full_fit <- pcr(y_full ~ x_full, scale = TRUE, ncomp = 5)

# Summary of the final model
summary(pcr_full_fit)


```

The final PCR model with 5 components provides a reduced-dimension view of the original data, retaining 77.67% of the information from the predictors and explaining 40.68% of the variability in Grad.Rate. This balance between dimensionality reduction and explanatory power is achieved without using all 17 original predictors, supporting a simpler, more interpretable model.

## Partial Least Squares (PLS)

Performing Partial Least Squares (PLS) Regression would be a logical next step to complement PCR analysis. While PCR focuses on reducing the dimensionality of the predictors without directly considering the response variable, PLS aims to find components that maximize the covariance between the predictors and the response, potentially leading to better predictive performance.

```{r}
# Fit PLS model using cross-validation
pls_fit <- plsr(Grad.Rate ~ ., data = train, scale = TRUE, validation = "CV")

# Summary to see cross-validation results
summary(pls_fit)

# Plot cross-validation MSE to choose the optimal number of components
validationplot(pls_fit, val.type = "MSEP")

```

From the cross-validation plot and the summary, we can conclude that the optimal number of components for the PLS model is 3. These components effectively balance predictive accuracy with model simplicity, capturing 65.49% of the variance in the predictors and 47.33% of the variance in Grad.Rate. Adding more components does not significantly reduce the RMSEP or improve the variance explained, indicating that 3 components should be sufficient for a well-performing and interpretable model.

```{r}
# Make predictions on the test set using 3 components
pls_pred = predict(pls_fit, x_test, ncomp = 3)

# Calculate Test Mean Squared Error (MSE) for PLS
mean((pls_pred - y_test)^2)
```

The MSE of 170.0129 reflects the average squared difference between the predicted and actual Grad.Rate values on the test set. Since this is a measure of prediction error, a lower value indicates better predictive performance. If we compare this PLS Test MSE (170.0129) to the Test MSE obtained from PCR model (181.3046), we can see that the PLS model has a lower MSE. This suggests that PLS is slightly better at predicting Grad.Rate than PCR, likely because PLS directly considers the response variable when deriving the components.

Now using M=3 and fitting the PLS on a full dataset

```{r}
# Fit PLS model on the full dataset using the optimal number of components (3)
pls_full_fit <- plsr(Grad.Rate ~ ., data = college.df, scale = TRUE, ncomp = 3)

# Summary of the final model
summary(pls_full_fit)

```

The final PLS model using 3 components achieves a good balance between simplifying the predictors and retaining explanatory power. It captures 64.21% of the variance in the predictors and 45.16% of the variance in Grad.Rate. Compared to PCR, PLS appears to offer better predictive performance (as seen by the lower Test MSE of 170.0129), making it the preferred approach for modeling graduation rates in this dataset.

The results suggest that these 3 components efficiently summarize the original data while maintaining a substantial amount of information, allowing for better predictions of Grad.Rate. This makes the final PLS model a solid choice for understanding and predicting graduation rates based on the available college data.
